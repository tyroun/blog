# 深度学习的数学

[TOC]

# 第一章 神经网络的思想

## 1.1 神经网络和深度学习

### 神经网络

1. 神经元形成网络。
2. 输入的和不超过阈值，则神经元不做出任何反应。
3. 输入的和超过阈值，则神经元做出反应（称为点火），向另外的神经元传递固定强度的信号
4. 在(2)和(3)中，每个输入对应的权重不一样。

## 1.2 神经元工作的数学表达

$$
\left\{
\begin{aligned}
& 无输出信号 (y=0): w_1x_1+w_2x_2+w_3x_3 < \theta \\
& 有输出信号 (y=1): w_1x_1+w_2x_2+w_3x_3 \geq \theta \
\end{aligned}
\right .
$$

$\theta$是神经元的阈值

转换成图形表示

<img src="../image/深度学习的数学/image-20211002111328401.png" alt="image-20211002111328401" style="zoom:67%;" />

上式可以用单位跃阶函数表达

$$
u(z) = \left\{
\begin{aligned}
& 0 (z<0) \\
& 1 (z\geq0) \\
\end{aligned}
\right.\\ y = u(w_1x_1+w_2x_2+w_3x_3-\theta)
$$

## 1.3 激活函数

神经元简化图

<img src="../image/深度学习的数学/image-20211002112146801.png" alt="image-20211002112146801" style="zoom:80%;" />

把上面的阶跃函数 u(v)一般化

$$ y = a(w_1x_1+w_2x_2+w_3x_3-\theta) \\ a称为激活函数(activation function)
$$

### Sigmoid 函数

把模型的描述函数用 sigmoid 函数表示

$$ f_\theta (x) = \frac{1}{1+exp(-\theta^\tau x)} $$

<img src="../image/深度学习的数学/图3-19.png" alt="image-20210926153556032" style="zoom:67%;" />

该函数的好处是处处可导

### 偏置

因为表达式里面的$\theta$ 带有负号，所以将-θ 替换成 b。b 称为偏置(bias)

![image-20211002113316760](../image/深度学习的数学/image-20211002113316760.png)

将 b 看成是常数为 1 的输入值。上式就可以改成向量的内积

$$ z = (w_1,w_2,w_3,b) \cdot (x_1,x_2,x_3,1)
$$

## 1.4 什么是神经网络

### 神经网络各层的职责

<img src="../image/深度学习的数学/image-20211002113904867.png" alt="image-20211002113904867" style="zoom:67%;" />

- **_输入层_** 负责读取输入信息，原样输出
- **_隐藏层/中间层_** 处理激活函数
- **_输出层_** 和隐藏层一样，但是输出结果

### 具体例子

4x3 像素的图像，手写数字 0 和 1。学习数据 64 张图象。每个像素只有 1bit 表示

<img src="../image/深度学习的数学/image-20211002115032564.png" alt="image-20211002115032564" style="zoom:50%;" />

这个网络，前一层和下一层所有神经元都有箭头连接。称为全连接层(fully connected layer)。

- 输入层总归 12 个神经单元，表示 12 个像素的值
- 输出层 2 个神经元输出，一个表示 0，一个表示 1。哪个输出值大表示是 0 或者是 1
- 隐藏层用于提取图像特征

# 第二章 神经网络的数学基础

## 2.1 神经网络所需的函数

### 正态分布的概率密度函数

$$
f(x) = \frac{1}{\sqrt{2\pi\sigma}} e ^{\frac{(x-u)^2}{2\sigma^2}}
$$

## 2.2 数列和递推关系式

就是说最后一层神经网络的结构，可以由之前几层数列之和求出来。

## 2.4 向量基础

### 张量(tensor)

法向量是垂直于面的向量，根据这个向量的方向（也就是法向），应力的方向和大小各不相同

当面的法向为 x、y、z 轴时，作用在面上的力依次用向量表示为

$$ \left(
\begin{matrix} \tau_{11} \\ \tau_{21} \\ \tau_{31} \end{matrix} \right) , \left(
\begin{matrix} \tau_{12} \\ \tau_{22} \\ \tau_{32}
\end{matrix}
\right) ,
\left(
\begin{matrix}
\tau_{13} \\ \tau_{23} \\ \tau_{33}
\end{matrix}
\right)
$$

将它们合并如下

$$
\left(
\begin{matrix}
\tau_{11} &\tau_{12} &\tau_{13}\\ \tau_{21} &\tau_{22} &\tau_{23}\\ \tau_{31} &\tau_{32} &\tau_{33}
\end{matrix}
\right)
$$

这个量叫张量。就是向量在各个坐标轴平面的投影

## 2.5 矩阵基础

- **_单位矩阵_**

  它是对角线上的元素 aii 为 1、其他元素为 0 的方阵，通常用 E 表示

$$
E=\left(
\begin{matrix}
1 &0 &0\\ 0 &1 &0\\ 0 &0 &1
\end{matrix}
\right)
$$

- **_矩阵乘积_**

<img src="../image/深度学习的数学/image-20211002160310205.png" alt="image-20211002160310205" style="zoom:67%;" />

AB$\neq$ BA

AE=EA=A

- **Hadamard 乘积**

  相同形状的矩阵 A、B，将相同位置的元素相乘，由此产生的矩阵称为矩阵 A、B 的 Hadamard 乘积，用 A⊙B 表示

- **转置矩阵**

  矩阵 A 的第 i 行第 j 列的元素与第 j 行第 i 列的元素交换。$A^t$

  <img src="../image/深度学习的数学/image-20211002160824008.png" alt="image-20211002160824008" style="zoom:67%;" />

## 2.6 导数基础

### 常用导数公式

<img src="../image/深度学习的数学/image-20211002160942827.png" alt="image-20211002160942827" style="zoom:80%;" />

### 导数性质

<img src="../image/深度学习的数学/image-20211002161026513.png" alt="image-20211002161026513" style="zoom:80%;" />

### 分数和 Sigmoid 函数的导数

​ <img src="../image/深度学习的数学/image-20211002161122664.png" alt="image-20211002161122664" style="zoom:67%;" />

$$ \sigma(x) = \frac{1}{1+e^{-x}} \\ \sigma^`(x) = \sigma(x)(1-\sigma(x))
$$

## 2.7 偏导基础

<img src="../image/深度学习的数学/image-20211002161556419.png" alt="image-20211002161556419" style="zoom:80%;" />

### 拉格朗日乘法

当 x^2^+y^2^=1 时，求 x+y 的最小值

L=f(x, y)-λg(x, y)=(x+y)-λ(x^2^+y^2^-1)

<img src="../image/深度学习的数学/image-20211002161709654.png" alt="image-20211002161709654" style="zoom:67%;" />

可得 x=y=λ= ±1/ 2 时，有最小值

## 2.8 误差反向传播法必需的链式法则

<img src="../image/深度学习的数学/image-20211002162141600.png" alt="image-20211002162141600" style="zoom:67%;" />

### 多变量情况

变量 z 为 u、v 的函数，如果 u、v 分别为 x、y 的函数，则 z 为 x、y 的函数

<img src="../image/深度学习的数学/image-20211002162326184.png" alt="image-20211002162326184" style="zoom:80%;" />

<img src="../image/深度学习的数学/image-20211002162407433.png" alt="image-20211002162407433" style="zoom:67%;" />

## 2.9 梯度下降法基础：多变量函数的近似公式

如果 x 作微小的变化，那么函数值 y 将会怎样变化

<img src="../image/深度学习的数学/image-20211002162544846.png" alt="image-20211002162544846" style="zoom:67%;" />

多变量情况

<img src="../image/深度学习的数学/image-20211002162700863.png" alt="image-20211002162700863" style="zoom:67%;" />

向量表示，3 个变量的近似公式可以看成以下 2 个向量的内积

<img src="../image/深度学习的数学/image-20211002163054082.png" alt="image-20211002163054082" style="zoom:67%;" />

∇ 通常读作 nabla

泰勒展开式

近似公式的一般化公式称为泰勒展开式。例如，在两个变量的情况下，这个公式如下所示

<img src="../image/深度学习的数学/image-20211002163158731.png" alt="image-20211002163158731" style="zoom:67%;" />

在泰勒展开式中，取出前三项，就得到以上的近似式

## 2.10 梯度下降法的含义与公式

函数 z=f(x, y)中，当 x 改变 ∆x, y 改变 ∆y 时，我们来考察函数 f(x, y)的值的变化 ∆z。

∆z=f(x+∆x, y+∆y)-f(x, y)

根据近似公式

$$ \Delta z = \frac{\partial{f(x,y)}}{\partial{x}} \Delta x + \frac{\partial{f(x,y)}}{\partial{y}} \Delta y $$

可以表示为两个向量的内积

$$ \Delta z = ( \frac{\partial{f(x,y)}}{\partial{x}}, \frac{\partial{f(x,y)}}{\partial{y}}) \cdot (\Delta x, \Delta y)
$$

要让函数 z 减小得最快，就是让 Δz 最大，也就是 cosθ=1，两个向量夹角为 180°

所以 (Δx, Δy) = - k $( \frac{\partial{f(x,y)}}{\partial{x}}, \frac{\partial{f(x,y)}}{\partial{y}})$ . k 是常数

在梯度下降法中，k 用 η 表示

### 哈密顿算子$\nabla$

∇ 称为哈密顿算子，其定义如下所示

$$ \nabla f = (
\frac{\partial{f}}{\partial{x_1}},\frac{\partial{f}}{\partial{x_2}},...,\frac{\partial{f}}{\partial{x_n}})
$$

所以

$$
(\Delta x_1,\Delta x_2,...,\Delta x_n) = - \eta \nabla f $$

# 第三章 神经网络的最优化

## 3.1 神经网络的参数和变量

### 数学符号

以如下神经网络为例

<img src="../image/深度学习的数学/image-20211002171823256.png" alt="image-20211002171823256" style="zoom:67%;" />

| 符号       | 含义                                                          |
| ---------- | ------------------------------------------------------------- |
| $x_i$      | 表示输入层的输入和输出变量                                    |
| $w^l_{ji}$ | l-1 层的第 i 个神经元 $\rightarrow$ j 层的第 j 个神经元的权重 |
| $z^l_j$    | l 层第 j 个神经元的加权输入变量                               |
| $b^l_j$    | l 层第 j 个神经元的偏置                                       |
| $a^l_j$    | l 层第 j 个神经元的输出                                       |

各符号含义如下图表示

<img src="../image/深度学习的数学/image-20211002172510056.png" alt="image-20211002172510056" style="zoom:67%;" />

神经元$a^l_j = a(z^i_j)$ a(z)为激活函数

<img src="../image/深度学习的数学/image-20211002172701358.png" alt="image-20211002172701358" style="zoom:67%;" />

### 神经元的图示方法

![image-20211002173334808](../image/深度学习的数学/image-20211002173334808.png)

两个神经元的关系可以表示如下

<img src="../image/深度学习的数学/image-20211002174458894.png" alt="image-20211002174458894" style="zoom:80%;" />

## 3.2 神经网络的变量关系式

### 输入层的关系式

$$
X_i = a^l_i
$$

### 隐藏层的关系式

![image-20211002174801697](../image/深度学习的数学/image-20211002174801697.png)

### 输出层的关系式

![image-20211002174832731](../image/深度学习的数学/image-20211002174832731.png)

## 3.3 学习数据和正解

就是设置$t_1 ,t_2$来表示 teacher，就是训练的结果

![image-20211002181551454](../image/深度学习的数学/image-20211002181551454.png)

## 3.4 神经网络的代价函数

![image-20211002181724733](../image/深度学习的数学/image-20211002181724733.png)

**全部数据的平方误差的总和就是代价函数**

### 参数和规模

以示例的神经网络模型为例

![image-20211002181905939](../image/深度学习的数学/image-20211002181905939.png)

参数总数 = （12x3+3) + (3x2 +2) = 47

**所以数据规模至少需要 47 个**

# 第四章 神经网络和误差反向传播法

## 4.2 梯度下降法回顾

因为如果参数很多，对代价函数求导再联立方程会非常复杂

比如上面那个例子，需要 47 个方程式

所以使用了梯度下降法

但是即使使用梯度下降法，还是需要对参数求偏导。如上式有 47 个参数，就要至少求 47 次偏导。还是计算量巨大

![image-20211002183856553](../image/深度学习的数学/image-20211002183856553.png)

![image-20211002183916434](../image/深度学习的数学/image-20211002183916434.png)

## 4.2 神经单元误差 $\delta^l_j$

### 引入符号$\delta^l_j$

$$ \begin{aligned} & \delta^l_j = \frac{\partial C}{\partial{z^l_j}} (l=2.3.... 表示layer, j表示第j个神经元) \\ & 代价函数式平方差函数C: \\
& C = \frac{1}{2} \{(t_1-a^3_1)^2+(t_2-a^3_2)^2\} \\ & 所以可以推得 \\ & \delta^2_1 = \frac{\partial C}{\partial{z^2_1}} ,
\delta^3_2 = \frac{\partial C}{\partial{z^3_2}} \end{aligned} $$

![image-20211002184751534](../image/深度学习的数学/image-20211002184751534.png)

利用$\delta^l_j$ 可以简化参数的偏导公式

$$
\begin{aligned}
& 因为 z_1^2 = w_{11}^2 x_1 + w_{12}^2 x_2 + ... + w_{112}^2 x_1 + b^2 \\
& \frac{\partial z^2_1}{\partial w_{11}^2} = x_1\\
& \frac{\partial C}{\partial{w_{11}^2}} = \frac{\partial C}{\partial{z_{1}^2}}\frac{\partial{z_{1}^2}}{\partial{w_{11}^2}} = \delta_1^2 x_1 \\
& 对于输入层，因为x_1 = a_1^1\\
& \frac{\partial C}{\partial{w_{11}^2}} = \delta_1^2 a_1 \\
& 同样可以推导出对偏置b得偏导 \\
& \frac{\partial C}{\partial{b_{1}^2}} = \delta_1^2  \\
\end{aligned}
$$

所以我们只要能求出每个神经元得误差$\delta^l_j$，就能求出梯度下降法要求的偏导

### $\delta^l_j$的含义与神经单元误差

我们来考虑一下将$\delta^l_j$称为神经单元误差的含义。从这个定义可知，$\delta^l_j$表示神经单元的加权输入 w 给平方误差带成的变化率。可以认为$\delta^l_j$表示与符合数据的理想状态的偏差

## 4.3 神经网络和误差反向传播法

### 通过递推关系越过导数计算

![image-20211002220218685](../image/深度学习的数学/image-20211002220218685.png)

### 计算输出层的$\delta^l_j$

计算例子得输出层神经元误差

$$
\delta^3_j = \frac{\partial C}{\partial{a_j^3}}*\frac{\partial{a_j^3}}{\partial{z_j^3}}=\frac{\partial C}{\partial{a_j^3}} a'(z_j^3)
$$

也就是说是代价函数对第三层得激活函数求导，再乘以激活函数对输出求导

代入具体数值

$$
\begin{aligned}
& C = \frac{1}{2}\{(t_1-a_1^3)^2+(t_2-a_2^3)^2\} \\
& \frac{\partial C}{\partial{a_j^3}} = a_1^3 - t_1 \\
& \delta_1^3 = (a_1^3 - t_1)a'(z_1^3) \\
& 激活函数a为Sigmoid函数\sigma(z)\\
& a'(z^3_1) = \sigma(z^3_1)(1-\sigma(z^3_1)) \\
& \delta_1^3 = (a_1^3 - t_1)\sigma(z^3_1)(1-\sigma(z^3_1)) \\
\end{aligned}
$$

### 中间层$\delta_j^l$的反向递推关系式

$\delta_j^l$可以通过简单的关系式，和下一层的$\delta_j^{l+1}$联系起来

<img src="../image/深度学习的数学/image-20211002222149435.png" alt="image-20211002222149435" style="zoom:67%;" />

<img src="../image/深度学习的数学/image-20211002222130985.png" alt="image-20211002222130985" style="zoom:67%;" />

因为

![image-20211002222455194](../image/深度学习的数学/image-20211002222455194.png)

可以得出

![image-20211002222510002](../image/深度学习的数学/image-20211002222510002.png)

<img src="../image/深度学习的数学/image-20211002222533737.png" alt="image-20211002222533737" style="zoom:67%;" />

一般式如下

![image-20211002222808893](../image/深度学习的数学/image-20211002222808893.png)

_m 为层 l+1 的神经单元的个数。l 为 2 以上的整数_

### 中间层的$\delta_j^l$不求导也可以得到值

先求出输出层$\delta_1^3,\delta_2^3$，在求出第 2 层$\delta_1^2,\delta_2^2,\delta_3^2$

这就是误差反向传播法

# 第五章 深度学习和卷积神经网络

## 5.1 基本原理

![image-20211002223833847](../image/深度学习的数学/image-20211002223833847.png)

### 卷积层

假设过滤器如下

<img src="../image/深度学习的数学/image-20211002224158635.png" alt="image-20211002224158635" style="zoom:50%;" />

用过滤器 S 扫描图像，然后把相似度汇总。做特征映射

![image-20211002224214725](../image/深度学习的数学/image-20211002224214725.png)

卷积层中的神经单元将这一卷积的结果作为输入信息。各神经单元将对应的卷积的值加上特征映射固有的偏置作为加权输入

![image-20211002224052159](../image/深度学习的数学/image-20211002224052159.png)

卷积层的各个神经单元通过激活函数来处理加权输入，并将处理结果作为神经单元的输出

![image-20211002224239493](../image/深度学习的数学/image-20211002224239493.png)

### 通过池化进行信息压缩

压缩的方法十分简单，只需要将卷积层神经单元划分为不重叠的 2×2 的区域，然后在各个区域中计算出代表值即可。本书中我们使用最有名的信息压缩方法最大池化（max pooling），就是取最大值。

![image-20211002224339932](../image/深度学习的数学/image-20211002224339932-16331858204301.png)

整体过程如下

![image-20211002224417981](../image/深度学习的数学/image-20211002224417981.png)

## 5.2 卷积神经网络的变量关系式

### 符号含义汇总表

| 位置     | 符号            | 含义                                                             |
| -------- | --------------- | ---------------------------------------------------------------- |
| 输入层   | $x_{ij}$        | i 行 j 列的输入，和输出相同                                      |
| 过滤器   | $w^{Fk}_{ij}$   | 第 k 个特征映射的过滤器，i 行 j 列的值                           |
| 卷积层   | $z^{Fk}_{ij}$   | 卷积层第 k 个子层，i 行 j 列的加权输入                           |
|          | $b^{Fk}$        | 卷积层第 k 个子层，i 行 j 列的偏置                               |
|          | $a^{Fk}_{ij}$   | 卷积层第 k 个子层，i 行 j 列的输出                               |
| 池化层   | $z^{Pk}_{ij}$   | 池化层第 k 个子层，i 行 j 列的输入                               |
|          | $a^{Pk}_{ij}$   | 池化层第 k 个子层，i 行 j 列的输出                               |
| 输出层   | $w_{k-ij}^{On}$ | 从池化层第 k 个子层的 i 行 j 列 指向 输出层的第 n 个神经元的权重 |
|          | $z_n^o$         | 输出层第 n 个神经元的加权输入                                    |
|          | $b_n^o$         | 输出层第 n 个神经元的偏置                                        |
|          | $a_n^o$         | 输出层第 n 个神经元的输出                                        |
| 学习数据 | $t_n$           |                                                                  |

### 输入层

以下关系式成立（a 的上标 I 为 Input 的首字母）

$$ a_{ij}^1 = x_{ij} $$

### 过滤器和卷积层

3 种过滤器，F 是 filter。过滤器也成为核(kernel)

![image-20211002225913614](../image/深度学习的数学/image-20211002225913614.png)

将输入层从左上角开始的 3×3 区域与过滤器 1 的对应分量相乘，得到下面的卷积值$c_{11}^{F1}$（c 为 convolution 的首字母）

![image-20211002230037762](../image/深度学习的数学/image-20211002230037762.png)

![image-20211002230101457](../image/深度学习的数学/image-20211002230101457.png)

依次滑动过滤器，用同样的方式计算求得卷积值$c_{12}^{F1},c_{13}^{F1}...$。这样一来，我们就得到了使用过滤器 1 的卷积的结果

将过滤器输出+偏置 -> 卷积层输入 $Z^{Fk}_{ij}$

![image-20211003083721035](../image/深度学习的数学/image-20211003083721035.png)

激励函数为 a(z)，卷积层的神经元输出$a^{Fk}_{ij}=a(z^{Fk}_{ij})$。卷积层所有神经元表示如下

![image-20211003084013002](../image/深度学习的数学/image-20211003084013002.png)

### 池化层

把 2x2 个神经单元压缩为 1 个，就形成了池化层

取 2x2 里面的最大值，就叫做最大池化法

<img src="../image/深度学习的数学/image-20211003084312414.png" alt="image-20211003084312414" style="zoom:67%;" />

### 输出层

输入

![image-20211003085017171](../image/深度学习的数学/image-20211003085017171.png)

输出

![image-20211003085035724](../image/深度学习的数学/image-20211003085035724.png)

### 代价函数$C_\tau$​

考虑 3 个输出的神经网络，设正解是$t_1,t_2,t_3$

$$ C = \frac{1}{2}\{(t_1 - a^o_1)^2+(t_2 - a^o_2)^2+(t_3 - a^o_3)^2\} \\ 全体学习数据的总和就是C_\tau C_\tau = sum^n_{i=1}Ci $$

### 用$\delta^l_i$表示**_==输出层==_**的梯度分量

以池化层第 2 个子层，2 行 1 列的神经元 -> 输出层 1 的偏导。如下图所示

![image-20211003180855631](../image/深度学习的数学/image-20211003180855631.png)

$$
\begin{aligned}
& \frac{\partial C}{\partial{w^{O1}_{2-21}}}=\frac{\partial C}{\partial{z^{O}_{1}}}\frac{\partial{z^{O}_{1}}}{\partial{w^{O1}_{2-21}}} = \delta_1^O a_{21}^{{P2}}\\
& \frac{\partial C}{\partial{b^{O}_{1}}} = \frac{\partial C}{\partial{z^{O}_{1}}}\frac{\partial{z^{O}_{1}}}{\partial{b^{O1}_{1}}} = \delta_1^O\\
\end{aligned}
$$

一般化如下式

<img src="../image/深度学习的数学/image-20211003181645815.png" alt="image-20211003181645815" style="zoom:80%;" />

### 用$\delta^l_i$表示**_==卷积层==_**的梯度分量

卷积层的输出如下, 每一个输出等于图像移动出来该 kernel 的叉积：

![image-20211003183412221](../image/深度学习的数学/image-20211003183412221.png)

每个输出对权重求偏导

![image-20211003183730869](../image/深度学习的数学/image-20211003183730869.png)

代价函数对卷积层某一个权重求导，可以看成是对每个卷积层输出求全导，然后每个输出去对某个权重求偏导

![image-20211003184317587](../image/深度学习的数学/image-20211003184317587.png)

代价函数对输出的偏导，就是误差$\delta_{ij}^{F1}$。所以上式可以改写成如下

![image-20211003184427583](../image/深度学习的数学/image-20211003184427583.png)

整个过程的关系图如下

![image-20211003184448174](../image/深度学习的数学/image-20211003184448174.png)

一般式如下

![image-20211003184543120](../image/深度学习的数学/image-20211003184543120.png)

![image-20211003185448110](../image/深度学习的数学/image-20211003185448110.png)

### 计算==输出层==的$\delta$

![image-20211003185649474](../image/深度学习的数学/image-20211003185649474-16332586102432-16332586109013.png)

因为 C 表达式如上，所以对输出求偏导结果如下

![image-20211003185703731](../image/深度学习的数学/image-20211003185703731.png)

综合上述 2 式，可得结果

![image-20211003200812439](../image/深度学习的数学/image-20211003200812439.png)

### 卷积层$\delta$ 的反向递推公式

以第一层卷积层的 1 行 1 列$\delta$为例

![image-20211003201150791](../image/深度学习的数学/image-20211003201150791.png)

关系图如下

![image-20211003201216212](../image/深度学习的数学/image-20211003201216212.png)

提取公因式后如下

![image-20211003201250332](../image/深度学习的数学/image-20211003201250332.png)

根据如上公式，输出层的输入 z 对 pool 层的输入 a 的偏导如下

![image-20211003201521955](../image/深度学习的数学/image-20211003201521955.png)

因为池化层用的是最大池化法

![image-20211003201615968](../image/深度学习的数学/image-20211003201615968.png)

所以

![image-20211003201650088](../image/深度学习的数学/image-20211003201650088.png)

![image-20211003201818297](../image/深度学习的数学/image-20211003201818297.png)

再把$\delta$ 代入

![image-20211003201917187](../image/深度学习的数学/image-20211003201917187.png)

![image-20211003201941810](../image/深度学习的数学/image-20211003201941810.png)

也就是说，通过后级的误差，推出前级的误差。这就是**_误差反向传播法_**
