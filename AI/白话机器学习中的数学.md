# 白话深度学习中的数学

{% raw %}

# 第二章 学习回归

## 2.1 设置问题

广告费用和点击量的比例

<img src="../image/machine learning math/图2-1.png" alt="image-20210921215658376" style="zoom:50%;" />

## 2.2 定义模型

$$ y = \theta_0 + \theta_1 x $$

在统计学领域，人们常常使用 θ 来表示未知数和推测值。采用 θ 加数字下标的形式，是为了防止当未知数增加时，表达式中大量出现 a、b、c、d…这样的符号

机器学习就是要求出正确的 θ0 和 θ1 的值

## 2.3 最小二乘法

把 y 换成$f_\theta (x)$

$$ f\_\theta (x) = \theta_0 + \theta_1 x$$

训练数据如下

| 广告费 x | 点击量 y |
| -------- | -------- |
| 58       | 374      |
| 70       | 385      |
| 81       | 375      |
| 84       | 401      |

<img src="../image/machine learning math/图2-6.png" alt="image-20210921221141201" style="zoom:50%;" />

如上图所示，如果有 n 个训练数据，误差和叫做**_目标函数_**，E($\theta$)的 E 是误差 Error 的首字母

$$ E(\theta)=\frac{1}{2} \displaystyle \sum^{n}_{i=1} (y^i - f_\theta (x^i)^2$$

x(i)和 y(i)中的 i 不是 i 次幂的意思，而是指第 i 个训练数据

因为是要求极值，$\frac{1}{2}$ 是故意加的，就为了微分时候可以把常数简化一点

把训练数据带入目标函数，设$θ_0$=1、$θ_1$=2

<img src="../image/machine learning math/公式2.3.3.png" alt="公式" style="zoom:50%;" />

让目标函数的值变小，用到最小二乘法

### 2.3.1 最速下降法

假设函数 g(x)

$$ g(x)=(x-1)^2$$

针对导数符号相反方向移动 x，这样自然求出最小值。这种方法被称为最速下降法(梯度下降法)

$$ x:= x - \eta\frac{dg(x)}{dx}$$

$\eta$被称为学习率。如果 η 较大，那么 x:=x-η(2x-2)会在两个值上跳来跳去，甚至有可能远离最小值。这就是发散状态。而当 η 较小时，移动量也变小，更新次数就会增加

现在对 E($\theta$)做偏微分，更新表达式如下

$$ \theta_0 := \theta_0 - \eta \frac{\partial E}{\partial \theta_0}$$

$$ \theta_1 := \theta_1 - \eta \frac{\partial E}{\partial \theta_1}$$

可以使用如下的方法来阶梯性地进行微分

$$ \frac{\partial u}{\partial \theta_0}=\frac{\partial u}{\partial v} \* \frac{\partial v}{\partial \theta_0}$$

<img src="../image/machine learning math/公式2.3.12.png" alt="image-20210922084651489" style="zoom:50%;" />

对$\theta_0$求偏导

$$
\begin{align*}
\frac{\partial u}{\partial \theta_0} &= \frac{\partial u}{\partial v} * \frac{\partial v}{\partial \theta_0} \\
&= \sum^n_{i=1}(v-y^i)*1 \\
&= \sum^n_{i=1}(f_\theta (x^i)-y^i)
\end{align*}
$$

对$\theta_1$求偏导

$$
\begin{align*}
\frac{\partial u}{\partial \theta_1} &= \frac{\partial u}{\partial v} * \frac{\partial v}{\partial \theta_1} \\
&= \sum^n_{i=1}(v-y^i)*x^i \\
&= \sum^n_{i=1}(f_\theta (x^i)-y^i) * x^i
\end{align*}
$$

根据上面的公式，所以对$\theta_0$ $\theta_1$ 的更新公式如下

$$
\begin{align*}
& \theta_0 := \theta_0 - \eta  \sum^n_{i=1}(f_\theta (x^i)-y^i) \\
& \theta_1 := \theta_1 - \eta  \sum^n_{i=1}(f_\theta (x^i)-y^i) x^i
\end{align*} $$

## 2.4 多项式回归

即在之前定义的模型基础上，加入更多次数的表达式

$$ f\_\theta (x) = \theta_0 + \theta_1 x + \theta_2 x^2 ...$$

但如果加入的次数太多，就会发生过拟合现象

对修改后的模型的目标函数求偏导。以 2 次为例

$$ \begin{align*} & \theta_0 := \theta_0 - \eta \sum^n_{i=1}(f_\theta (x^i)-y^i) \\ & \theta_1 := \theta_1 - \eta
\sum^n_{i=1}(f_\theta (x^i)-y^i) x^i \\ & \theta_2 := \theta_2 - \eta \sum^n_{i=1}(f_\theta (x^i)-y^i) x^{i^2} \\
\end{align*} $$

## 2.5 多重回归

变量不止有 x，有多个变量的情况

$$ f\_\theta (x_1,x_2,x_3) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3$$

如果变量有 n 个，就需要用到向量表达式

$$ \theta = \left[\begin{matrix} \theta_0 \\\theta_1 \\\theta_2 \\\vdots \\\theta_n \end{matrix}\right] x=
\left[\begin{matrix} 1 \\x_1 \\x_2 \\\vdots \\x_n \end{matrix}\right]$$

所以转置$\theta$ 后

$$ f\_\theta (x) = \theta ^\tau x$$

所以目标函数求偏导，对第 j 个参数表达式更新如下

$$ \theta_j := \theta_j - \eta \sum^n_{i=1}(f_\theta (x^i)-y^i) x^i_j $$

## 2.6 随机梯度下降法

最速下降法容易陷入局部最优解

<img src="../image/machine learning math/图2-19.png" alt="image-20210924172919615" style="zoom:50%;" />

不同于在目标函数中取所有样本值的和，改用取随机 1 个或者 m 个值得和

$$ \theta_j := \theta_j - \eta \sum_{k\in K}(f_\theta (x^i)-y^i) x^i_j $$

# 第三章 学习分类

## 3.1 设置问题

根据宽高判断图片是横向还是竖向，白色是竖向

<img src="../image/machine learning math/图3-4.png" alt="image-20210926134518811" style="zoom:50%;" />

问题用函数表示如下，下面的函数称为判别函数。 -1 表示纵向白色的点

$$ f_w (x) = \left\{ \begin{aligned} 1 & &(w_1x_1+w_2x_2 \geq 0)  \\ -1 & &(w\cdot x < 0)  \\ \end{aligned} \right. $$

## 3.2 内积

分类的目的是要在坐标轴上找到一条类似虚线的线。这条线的法向向量就是权重向量。

如果把图上的每一个点都看成是一个向量，权重向量和白色的向量的夹角都是<90°，所以内积都小于 0。和黑色点的夹角都是>90°，所以内积都>0。

<img src="../image/machine learning math/图3-8.png" alt="image-20210926142613775" style="zoom:50%;" />

## 3.3 感知机

如下图接收多个输入，并输出每个输入和权重积的总和。这样的模型叫做感知机，在深度学习里也叫做神经元

<img src="../image/machine learning math/图3-9.png" alt="感知机" style="zoom:67%;" />

### 权重向量的更新表达式

如何通过训练迭代的方式更新权重向量。注意这里的 i 表示第 i 个训练数据。

$$ w：= \left\{ \begin{aligned} & w + y^ix^i & &(f_x (x^i) \neq y^i )  \\ & w & &(f_x (x^i) = y^i)  \\ \end{aligned}
\right. $$

上面这个公式意思就是，如果把 w 代入判别函数，和结果一致就不更新 w，和结果不一致就把(w1,w2) + ($x_i$ , $y_i$)

<img src="../image/machine learning math/图3-16.png" alt="image-20210926151527353" style="zoom:67%;" />

## 3.4 线性可分

如下图这样的分类问题，无法用一条直线划分。称为线性不可分

<img src="../image/machine learning math/图3-18.png" alt="image-20210926151829848" style="zoom:67%;" />

## 3.5 逻辑回归

把分类问题当作概率来考虑。

### 3.5.1 sigmoid 函数

把模型的描述函数用 sigmoid 函数表示

$$ f_\theta (x) = \frac{1}{1+exp(-\theta^\tau x)} $$

<img src="../image/machine learning math/图3-19.png" alt="image-20210926153556032" style="zoom:67%;" />

### 3.5.2 决策边界

把横向图像得概率表示如下,表示 y=1 是的概率函数是$f_\theta (x)$

$$ P(y=1|x) = f_\theta (x)
$$

$$ y = \left\{ \begin{aligned} &1 & &(f_\theta (x) \geq 0.5 )  \\ & 0 & &(f_\theta (x) < 0.5)  \\ \end{aligned} \right.
$$

为了求得正确的参数$\theta$ ，就需要参考回归的方法，定义目标函数 E($\theta$)，然后进行微分

## 3.6 似然函数

- y=1 的时候，我们希望概率 P(y=1|x)是最大的
- y=0 的时候，我们希望概率 P(y=0|x)是最大的

如果有 6 个训练数据，联合概率就是 6 个概率相乘。如果有 n 个训练数据，其中 y=1 的训练数据有 i 个。则一般表达式如下

$$ L(\theta) = \prod^n_{i=1} P(y^i=1|x^i)^{y^i}P(y^i=0|x^i)^{1-y^i} $$

L($\theta$) 被称为 Likelihood 函数，要求出$\theta$ 使得 L($\theta$)的值最大

## 3.7 对数似然函数

对 L($\theta$)去 log 后，可以把乘法变加法，但是单调性不会改变。这个函数就作**_目标函数_**

$$ \log L(\theta) = \sum^n_{i=1} (y^i \log f_\theta (x^i)+(1-y^i)\log (1-f_\theta (x^i)))
$$

### 3.7.1 似然函数微分

$$ \begin{align} 设: \\
u & = \log L(\theta) \\
v & = f_\theta (x) \\
\frac{\partial u}{\partial v} & = \sum^n_{i=1} (\frac{y^i}{v}-\frac{1-y^i}{1-v}) \\
\frac{\partial v}{\partial \theta_j} &= v(1-v) \cdot x_j \\
\frac{\partial u}{\partial \theta_j} &= \sum^n_{j=1} (y^i - f_\theta (x^i)) x^i_j \\
\theta_j &:= \theta_j + \eta \sum^n_{j=1} (y^i - f_\theta (x^i)) x^i_j
\end{align}
$$

最后这个就是根据回归算出的，$\theta_j$的更新公式。似然函数的微分就是不停地求导

## 3.8 线性不可分

就是要增加$x_1$ $x_2$ 的次数，引入$\theta_3 \theta_4$这些系数来对应$x^2_1 x^2_2$

# 第四章 评估已经建立的模型

## 4.1 模型评估

如果有多个变量，无法直接通过作图判断描述模型的函数是否正确

需要有一个方法评估模型

## 4.2 交叉验证

### 4.2.1 回归问题的验证

将数据集分为测试数据和训练数据。

通过求误差平方的均值 MSE 来判断，MSE 越小说明模型越好

### 4.2.2 分类问题验证

<img src="../image/machine learning math/表4-2.png" alt="image-20210926172852766" style="zoom:50%;" />
$$
Accuracy = \frac{TP+TN}{TP+FP+FN+TN}
$$
表示一个测试集中被正确分类的百分比

### 4.2.3 精确率(precision)和召回率(Recall)

Precision 表示在分类为正的数据里，分类正确的百分比

$$ Precision = \frac{TP}{TP+FP} $$

Recall 表示在所有正确分类的数据里，分类为正正确的百分比

$$ Recall = \frac{TP}{TP+FN} $$

### 4.2.4 F 值

现有两个模型

| 模型   | 精确率 | 召回率 | 平均值 |
| ------ | ------ | ------ | ------ |
| 模型 A | 0.6    | 0.39   | 0.495  |
| 模型 B | 0.02   | 1.0    | 0.51   |

如果看平均是 B 好，但是 B 对所有 positive 的识别率太低了

$$ Fmeasure = \frac{2}{\frac{1}{Precision}+\frac{1}{Recall}} $$

这样任何一个值小，都会影响结果

加权 F 值

$$ WeightedFmeasure = \frac{(1+\beta^2)*Precision*Recall}{\beta^2*Precision+Recal} $$

当数据不平衡的时候，使用数量少的作为判断条件。比如 Positive 少，那就用 Precision

### 4.2.5 K 折交叉验证

- 将全部训练数据分成 K 份，K-1 作为训练数据，1 份作为测试数据
- 每次更换训练数据和测试数据，重复进行 K 次交叉验证
- 最后计算 K 个精度的平均值，把它作为最终的精度

## 4.3 正则化

### 4.3.1 过拟合

只能拟合训练数据的状态被称为过拟合

避免方法

- 增加全部训练数据的数量
- 使用简单的模型
- 正则化

### 4.3.2 正则化的方法

对于回归问题，在目标函数后面增加正则化项

$$ \begin{aligned} E(\theta) & =\frac{1}{2} \displaystyle \sum^{n}_{i=1} (y^i - f_\theta (x^i)^2 +R(\theta) \\ & =
\frac{1}{2} \displaystyle \sum^{n}_{i=1} (y^i - f_\theta (x^i)^2 +\frac{\lambda}{2} \sum^m_{j=1} \theta^2_j
\end{aligned} $$

其中 m 是参数个数-1，去掉$\theta_0$

λ 是决定正则化项影响程度的正的常数。这个值需要我们自己来定。

### 4.3.3 正则化的效果

针对单个$\theta$ 的情况

<img src="../image/machine learning math/图4-17.png" alt="image-20210927095738657" style="zoom:50%;" />

本来是在 θ1=4.5 处最小，现在是在 θ1=0.9 处最小

**_正则化的效果_**： 它可以防止参数变得过大，有助于参数接近较小的值

λ 越大，对该参数的惩罚就越大

### 4.3.4 分类的正则化

还是一样的，对于对数似然函数，增加正则化项

$$ \log L(\theta) = -\sum^n_{i=1} (y^i \log f_\theta (x^i)+(1-y^i)\log (1-f_\theta (x^i))) + \frac{\lambda}{2} \sum^m_
{j=1} \theta^2_j $$

因为 logL(θ)是取最大值，所以上式要加个负号

### 4.3.5 包含正则化项的微分

$$ \begin{aligned} & \frac{\partial R(\theta)}{\partial \theta_j} = \lambda \theta_j \\ & \frac{\partial E(\theta)
}{\partial \theta_j} = \sum^n_{i=1}(f_\theta (x^i)-y^i)x^i_j + \lambda \theta_j \\ & \theta_j:= \theta_j + \eta (\sum^n_
{i=1}(f_\theta (x^i)-y^i)x^i_j + \lambda \theta_j) \\ & \theta_0 : = \theta_j + \eta (\sum^n_{i=1}(f_\theta (x^i)-y^i)
x^i_j) \\ \end{aligned} $$

以上方法称为**L2**正则化，特点是不会吧参数变成 0 个

L1 正则化的 R 项如下，特点是会把不需要的参数变成 0

$$ R(\theta) = \lambda \sum^m_{i=1} |\theta_i| $$

## 4. 4 学习曲线

### 4.4.1 欠拟合

模型对于要解决的问题太简单了，无法对训练数据得出正确的结果

### 4.4.2 区分过拟合与欠拟合

欠拟合的精度曲线如下。黑点是训练样本，白点是测试样本

随着训练样本的增加，训练精度下降，同时测试精度上升到一定值以后陷入瓶颈

<img src="../image/machine learning math/图4-27.png" alt="image-20210927101555032" style="zoom:50%;" />

过拟合曲线如下。

随着训练样本的增加，训练精度不变，但是测试精度上升到一定值以后陷入瓶颈。和训练精度有差距

<img src="../image/machine learning math/图4-28.png" alt="image-20210927101830437" style="zoom:50%;" />

{% endraw %}
